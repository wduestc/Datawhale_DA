{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:gb18030\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "#启动spark\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"test\").getOrCreate()\n",
    "#sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取json文件格式\n",
    "df = spark.read.json('people.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 打印模式\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选择多列\n",
    "df.select(df.name, df.age + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 条件过滤\n",
    "df.filter(df.age > 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组聚合\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#排序\n",
    "df.sort(df.age.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 多列排序\n",
    "df.sort(df.age.desc(), df.name.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+\n",
      "|username| age|\n",
      "+--------+----+\n",
      "| Michael|null|\n",
      "|    Andy|  30|\n",
      "|  Justin|  19|\n",
      "+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对列进行重命名\n",
    "df.select(df.name.alias(\"username\"), df.age).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 利用反射机制推断出rdd模式\n",
    "from pyspark.sql.types import Row\n",
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['name']= x[0]\n",
    "    rel['age'] = x[1]\n",
    "    return rel\n",
    "peopleDF = sc.textFile('people.txt').map(lambda line:line.split(',')).map(lambda x:Row(**f(x))).toDF()\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "personDF = spark.sql(\"select * from people\")\n",
    "personDF.rdd.map(lambda t: \"Name:\" + t[0] + \",\" + \"Age:\" + t[1]).foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用编程方式定义RDD\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "peopleRDD = sc.textFile(\"people.txt\")\n",
    "schemaString = \"name age\"\n",
    "# 根据模式字符串生成模式\n",
    "fields = list(map(lambda fieldName: StructField(fieldName, StringType(), nullable=True), schemaString.split(\" \")))\n",
    "schema = StructType(fields)\n",
    "rowRDD = peopleRDD.map(lambda line:line.split(',')).map(lambda attributes: Row(attributes[0], attributes[1]))\n",
    "peopleDF = spark.createDataFrame(rowRDD, schema)\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    "results = spark.sql(\"select * from people\")\n",
    "results.rdd.map(lambda attributes: \"name: \" + attributes[0] + \", \" +\"age:\" + attributes[1]).foreach(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd保存成文件\n",
    "peopleDF = spark.read.format(\"json\").load(\"people.json\")\n",
    "# 暂时没找出原因 后续再查一下\n",
    "# peopleDF.rdd.saveAsTextFile(\"file:///C:/Users/wd/Documents/GitHub/Datawhale_DA/newpeople1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SparkStreaming程序基本步骤\n",
    "#1.通过创建输入DStream来定义输入源\n",
    "#2.通过对DStream应用转换操作和输出操作来定义流计算。\n",
    "#3.用streamingContext.start()来开始接收数据和处理流程。\n",
    "#4.通过streamingContext.awaitTermination()方法来等待处理结束（手动结束或因为错误而结束）。\n",
    "#5.可以通过streamingContext.stop()来手动结束流计算进程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "# 每隔一秒自动执行一次流计算\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果是编写独立的SparkStreaming程序 \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "#conf = SparkConf()\n",
    "#conf.setAppName(\"TestDStream\")\n",
    "#表示在本地运行 且启动两个进程\n",
    "#conf.setMaster(\"local[2]\")\n",
    "#sc = SparkContext(conf=conf)\n",
    "ssc = StreamingContext(sc, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
